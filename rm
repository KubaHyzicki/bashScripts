#!/usr/bin/env bash
#_source

#hmmm pomysł wyszedł z wypadku użycia rm -rf na złym folderze, a dokładnie miał to być subfolder...nic się strasznego nie stało, ale na przyszłość można zrobić rm, który oprócz usunięcia robi backup ostatnio usuniętych rzeczy

#dla dużych rzeczy kopiowanie może być problematyczne więc zamiast kopiować można przenosić do np. ~/.rm_backup/
#a dokładniej:

if [[ $1 == "--help" ]]
then
	echo "Overwrites rm command with copying dirs/files to backup folder(cleans automaticly after exceeding amount of parametrised backup files"
	exit
fi

export backup=~/Documents/rm_backup
max_backups=3

clean(){
	max=$1
	cd $backup
	count=0
	echo `ls | wc -l`
	if [[ `ls | wc -l` -le $max ]]
	then
		echo "no need of cleanup"
		exit
	fi
	for x in `ls -t`
	do
		if [[ $count -ge $max ]]
		then
			echo "rm $x"
			#rm -rf $x
		else
			echo "not $x"
			count=$(($count+1))
		fi
	done
	cd - &> /dev/null
}

remove(){
	#args=$2
	mv $1 $backup/$1
}

if [[ ! -d "$backup" ]]
then
	mkdir $backup
fi
clean $max_backups
if false ;then 
args=''
if [[ ${1::1} == "-" ]]
then
	args=${1:1}
	shift
	echo "I actually do not need any args..."
fi
for x in $@
do
	remove $x $args
done
fi
#dodać licznik plików w folderze backupu by usuwało najwcześniej dodane jeśli ilość w folderze przekroczy np. 10
#clean mógłby się wywoływać po komendzie(z zapamiętaną ilością starych obiektów do usunięcia) w oddzielonym przez & procesie dla szybszego wykonywania polecenia
#dodać obsługę "*" i ogólnie wyrażeń regularnych do wykrywania plików do usunięcia...możliwe że da się tu wykorzystać lsd

